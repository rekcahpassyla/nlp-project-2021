@inproceedings{10.1145/3383902.3383909,
author = {Hajek, Petr and Barushka, Aliaksandr},
title = {A Comparative Study of Machine Learning Methods for Detection of Fake Online Consumer Reviews},
year = {2019},
isbn = {9781450371704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383902.3383909},
doi = {10.1145/3383902.3383909},
abstract = {Online product reviews provide valuable information for consumer decision making. Customers increasingly rely on the reviews and consider them a trusted source of information. For businesses, it is therefore tempting to purchase fake reviews because competitive advantage can be easily achieved by producing positive or negative fake reviews. Machine learning methods have become a critical tool to automatically identify fake reviews. Recently, deep neural networks have shown promising detection accuracy. However, there have been no studies which compare the performance of state-of-the-art deep learning approaches with traditional machine learning methods, such as Na\"{\i}ve Bayes, support vector machines or decision trees. The aim of this study is to examine the performance of several machine learning methods used for the detection of positive and negative fake consumer reviews. Here we show that deep neural networks, including convolutional neural networks and long short term memory, significantly outperform the traditional machine learning methods in terms of accuracy while preserving desirable time performance.},
booktitle = {Proceedings of the 2019 3rd International Conference on E-Business and Internet},
pages = {18–22},
numpages = {5},
keywords = {classification, machine learning, Fake, deep learning, reviews},
location = {Prague, Czech Republic},
series = {ICEBI 2019}
}
@article{Tang_2020,
	doi = {10.1088/1742-6596/1651/1/012055},
	url = {https://doi.org/10.1088/1742-6596/1651/1/012055},
	year = 2020,
	month = {nov},
	publisher = {{IOP} Publishing},
	volume = {1651},
	pages = {012055},
	author = {Haoxing Tang and Hui Cao},
	title = {A review of research on detection of fake commodity reviews},
	journal = {Journal of Physics: Conference Series},
	abstract = {With the development of the Internet era, more and more consumers are shopping online. Product reviews can influence consumers’ consumption decisions. Fake review detection is a valuable research. From the perspective of research data content, it can be divided into two categories: review content based and review metadata; from the perspective of fake review detection method, it can be divided into four categories: rule-based, graph-based model, machine-learning based and deep learning based. Common data sets are also introduced.}
}
@article{Hjek2020FakeCR,
  title={Fake consumer review detection using deep neural networks integrating word embeddings and emotion mining},
  author={P. H{\'a}jek and Aliaksandr Barushka and M. Munk},
  journal={Neural Computing and Applications},
  year={2020},
  pages={1-16}
}
@inproceedings{10.1145/1871437.1871557,
author = {Lim, Ee-Peng and Nguyen, Viet-An and Jindal, Nitin and Liu, Bing and Lauw, Hady Wirawan},
title = {Detecting Product Review Spammers Using Rating Behaviors},
year = {2010},
isbn = {9781450300995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871437.1871557},
doi = {10.1145/1871437.1871557},
abstract = {This paper aims to detect users generating spam reviews or review spammers. We identify several characteristic behaviors of review spammers and model these behaviors so as to detect the spammers. In particular, we seek to model the following behaviors. First, spammers may target specific products or product groups in order to maximize their impact. Second, they tend to deviate from the other reviewers in their ratings of products. We propose scoring methods to measure the degree of spam for each reviewer and apply them on an Amazon review dataset. We then select a subset of highly suspicious reviewers for further scrutiny by our user evaluators with the help of a web based spammer evaluation software specially developed for user evaluation experiments. Our results show that our proposed ranking and supervised methods are effective in discovering spammers and outperform other baseline method based on helpfulness votes alone. We finally show that the detected spammers have more significant impact on ratings compared with the unhelpful reviewers.},
booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
pages = {939–948},
numpages = {10},
keywords = {review spammer, spamming behavior},
location = {Toronto, ON, Canada},
series = {CIKM '10}
}
@inproceedings{10.1145/1871437.1871669,
author = {Jindal, Nitin and Liu, Bing and Lim, Ee-Peng},
title = {Finding Unusual Review Patterns Using Unexpected Rules},
year = {2010},
isbn = {9781450300995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871437.1871669},
doi = {10.1145/1871437.1871669},
abstract = {In recent years, opinion mining attracted a great deal of research attention. However, limited work has been done on detecting opinion spam (or fake reviews). The problem is analogous to spam in Web search [1, 9 11]. However, review spam is harder to detect because it is very hard, if not impossible, to recognize fake reviews by manually reading them [2]. This paper deals with a restricted problem, i.e., identifying unusual review patterns which can represent suspicious behaviors of reviewers. We formulate the problem as finding unexpected rules. The technique is domain independent. Using the technique, we analyzed an Amazon.com review dataset and found many unexpected rules and rule groups which indicate spam activities.},
booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
pages = {1549–1552},
numpages = {4},
keywords = {review spam, unexpected patterns, reviewer behavior},
location = {Toronto, ON, Canada},
series = {CIKM '10}
}
@inproceedings{conf/icwsm/MukherjeeV0G13,
  added-at = {2013-08-20T00:00:00.000+0200},
  author = {Mukherjee, Arjun and Venkataraman, Vivek and Liu, Bing and Glance, Natalie S.},
  biburl = {https://www.bibsonomy.org/bibtex/246aff5394336b0564025bfd386938fac/dblp},
  booktitle = {ICWSM},
  crossref = {conf/icwsm/2013},
  editor = {Kiciman, Emre and Ellison, Nicole B. and Hogan, Bernie and Resnick, Paul and Soboroff, Ian},
  ee = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/view/6006},
  interhash = {d9e3ab88bf2819a40b8d03602795642a},
  intrahash = {46aff5394336b0564025bfd386938fac},
  isbn = {978-1-57735-610-3},
  keywords = {dblp},
  publisher = {The AAAI Press},
  timestamp = {2013-08-21T11:37:38.000+0200},
  title = {What Yelp Fake Review Filter Might Be Doing?},
  url = {http://dblp.uni-trier.de/db/conf/icwsm/icwsm2013.html#MukherjeeV0G13},
  year = 2013
}
@inproceedings{conf/icwsm/FeiMLHCG13,
  added-at = {2013-08-21T00:00:00.000+0200},
  author = {Fei, Geli and Mukherjee, Arjun and Liu, Bing and Hsu, Meichun and Castellanos, Malú and Ghosh, Riddhiman},
  biburl = {https://www.bibsonomy.org/bibtex/2549a92d85bdaebe586919212d4dd85e2/dblp},
  booktitle = {ICWSM},
  crossref = {conf/icwsm/2013},
  editor = {Kiciman, Emre and Ellison, Nicole B. and Hogan, Bernie and Resnick, Paul and Soboroff, Ian},
  ee = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/view/6069},
  interhash = {5cb07bb7112b5858608f93a5a7754336},
  intrahash = {549a92d85bdaebe586919212d4dd85e2},
  isbn = {978-1-57735-610-3},
  keywords = {dblp},
  publisher = {The AAAI Press},
  timestamp = {2013-08-22T11:39:38.000+0200},
  title = {Exploiting Burstiness in Reviews for Review Spammer Detection.},
  url = {http://dblp.uni-trier.de/db/conf/icwsm/icwsm2013.html#FeiMLHCG13},
  year = 2013
}
@inproceedings{10.1145/2487575.2487580,
author = {Mukherjee, Arjun and Kumar, Abhinav and Liu, Bing and Wang, Junhui and Hsu, Meichun and Castellanos, Malu and Ghosh, Riddhiman},
title = {Spotting Opinion Spammers Using Behavioral Footprints},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487580},
doi = {10.1145/2487575.2487580},
abstract = {Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or to demote some target products. In recent years, fake review detection has attracted significant attention from both the business and research communities. However, due to the difficulty of human labeling needed for supervised learning and evaluation, the problem remains to be highly challenging. This work proposes a novel angle to the problem by modeling spamicity as latent. An unsupervised model, called Author Spamicity Model (ASM), is proposed. It works in the Bayesian setting, which facilitates modeling spamicity of authors as latent and allows us to exploit various observed behavioral footprints of reviewers. The intuition is that opinion spammers have different behavioral distributions than non-spammers. This creates a distributional divergence between the latent population distributions of two clusters: spammers and non-spammers. Model inference results in learning the population distributions of the two clusters. Several extensions of ASM are also considered leveraging from different priors. Experiments on a real-life Amazon review dataset demonstrate the effectiveness of the proposed models which significantly outperform the state-of-the-art competitors.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {632–640},
numpages = {9},
keywords = {deceptive and fake reviewer detection, abuse, opinion spam},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}
@inproceedings{10.1145/1341531.1341560,
author = {Jindal, Nitin and Liu, Bing},
title = {Opinion Spam and Analysis},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341560},
doi = {10.1145/1341531.1341560},
abstract = {Evaluative texts on the Web have become a valuable source of opinions on products, services, events, individuals, etc. Recently, many researchers have studied such opinion sources as product reviews, forum posts, and blogs. However, existing research has been focused on classification and summarization of opinions using natural language processing and data mining techniques. An important issue that has been neglected so far is opinion spam or trustworthiness of online opinions. In this paper, we study this issue in the context of product reviews, which are opinion rich and are widely used by consumers and product manufacturers. In the past two years, several startup companies also appeared which aggregate opinions from product reviews. It is thus high time to study spam in reviews. To the best of our knowledge, there is still no published study on this topic, although Web spam and email spam have been investigated extensively. We will see that opinion spam is quite different from Web spam and email spam, and thus requires different detection techniques. Based on the analysis of 5.8 million reviews and 2.14 million reviewers from amazon.com, we show that opinion spam in reviews is widespread. This paper analyzes such spam activities and presents some novel techniques to detect them},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {219–230},
numpages = {12},
keywords = {review spam, opinion spam, review analysis, fake reviews},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}
@inproceedings{10.1145/1242572.1242759,
author = {Jindal, Nitin and Liu, Bing},
title = {Review Spam Detection},
year = {2007},
isbn = {9781595936547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1242572.1242759},
doi = {10.1145/1242572.1242759},
abstract = {It is now a common practice for e-commerce Web sites to enable their customers to write reviews of products that they have purchased. Such reviews provide valuable sources of information on these products. They are used by potential customers to find opinions of existing users before deciding to purchase a product. They are also used by product manufacturers to identify problems of their products and to find competitive intelligence information about their competitors. Unfortunately, this importance of reviews also gives good incentive for spam, which contains false positive or malicious negative opinions. In this paper, we make an attempt to study review spam and spam detection. To the best of our knowledge, there is still no reported study on this problem.},
booktitle = {Proceedings of the 16th International Conference on World Wide Web},
pages = {1189–1190},
numpages = {2},
keywords = {review spam, product reviews, opinion spam},
location = {Banff, Alberta, Canada},
series = {WWW '07}
}
@inproceedings{10.1145/2187836.2187863,
author = {Mukherjee, Arjun and Liu, Bing and Glance, Natalie},
title = {Spotting Fake Reviewer Groups in Consumer Reviews},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187863},
doi = {10.1145/2187836.2187863},
abstract = {Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or demote some target products. For reviews to reflect genuine user experiences and opinions, such spam reviews should be detected. Prior works on opinion spam focused on detecting fake reviews and individual fake reviewers. However, a fake reviewer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging as they can take total control of the sentiment on the target product due to its size. This paper studies spam detection in the collaborative setting, i.e., to discover fake reviewer groups. The proposed method first uses a frequent itemset mining method to find a set of candidate groups. It then uses several behavioral models derived from the collusion phenomenon among fake reviewers and relation models based on the relationships among groups, individual reviewers, and products they reviewed to detect fake reviewer groups. Additionally, we also built a labeled dataset of fake reviewer groups. Although labeling individual fake reviews and reviewers is very hard, to our surprise labeling fake reviewer groups is much easier. We also note that the proposed technique departs from the traditional supervised learning approach for spam detection because of the inherent nature of our problem which makes the classic supervised learning approach less effective. Experimental results show that the proposed method outperforms multiple strong baselines including the state-of-the-art supervised classification, regression, and learning to rank algorithms.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {191–200},
numpages = {10},
keywords = {opinion spam, group opinion spam, fake review detection},
location = {Lyon, France},
series = {WWW '12}
}