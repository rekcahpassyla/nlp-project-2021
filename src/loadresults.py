# This script loads the results.hdf5 file that was generated by running
# classify.py in non-training mode (evaluation), and parses the values
# into sensible structures.

import sys
sys.path.append('..')

import os
import pandas as pd
import numpy as np

import inputoutput as io

# translate our tags to pretty names
names = {
    'bert-base-uncased': 'BERT-base',
    'distilbert-base-uncased': 'DistilBERT',
    'uk': 'UK',
    'us': 'US',
    'all': 'Combined',
}


def gen_model_name(modelname):
    # convert us_bert-base-uncaed to BERT-base/US training set
    loc, model = modelname.split("_")
    return f"{names[model]}/{names[loc]}"


def gen_test_setname(testfile):
    # convert test_set_all to Combined test set
    testfile = testfile.replace("_features", '')
    if testfile == 'new_test_set':
        return 'Adversarial'
    loc = testfile.split("_")[-1]
    return names[loc]


class Record:
    dps = 3   # number of decimal places to round to

    def __init__(self, modelname, trainingset, predictions, labels, data):
        # eg. uk_bert-base-cased
        self.modelname = modelname
        # eg. test_set_uk
        self.trainingset = trainingset
        # these are 3 iterables of the same length
        # and whose indexes match each other
        # eg. the data in inndex 5 has its label in index 5
        # and its prediction in index 5
        # the predictions come from the saved modelname tested on trainingset.
        self.predictions = predictions
        # the data and labels comes from the file name represented by trainingset
        self.labels = labels
        self.data = np.array(data)

    @property
    def N(self):
        # return number of predictions
        return len(self.predictions)

    @property
    def Npos(self):
        return self.labels[self.labels==1].size

    @property
    def Nneg(self):
        return self.labels[self.labels==0].size

    def accuracy(self):
        return (self.predictions == self.labels).mean()

    def misclassified(self, label=None):
        # find all the data items which don't match
        # if label is set to None or any value other than 0 or 1,
        # then will return all mismatches
        # otherwise will return mismatches with the specific label
        unmatched = self.predictions != self.labels

        if label == 1:
            unmatched = unmatched & (self.labels == 1)
        elif label == 0:
            unmatched = unmatched & (self.labels == 0)

        return self.data[unmatched]

    def positives(self, _type=True):
        # items classified as positive, true or false according to type
        if _type:
            pos = (
                (self.labels == 1) &
                (self.predictions == 1)
            )
        else:
            pos = (
                    (self.labels == 0) &
                    (self.predictions == 1)
            )
        return np.nonzero(pos)[0].size

    def negatives(self, _type=True):
        # items classified as negatives, true or false according to type
        if _type:
            neg = (
                    (self.labels == 0) &
                    (self.predictions == 0)
            )
        else:
            neg = (
                    (self.labels == 1) &
                    (self.predictions == 0)
            )
        return np.nonzero(neg)[0].size

    @property
    def precision(self):
        # true positives / (true positives + false positives)
        return np.round(self.positives(True) / (self.positives(True) + self.positives(False)), self.dps)

    @property
    def recall(self):
        # true positives / (true positives + false negatives)
        return np.round(self.positives(True) / (self.positives(True) + self.negatives(False)), self.dps)

    @property
    def specificity(self):
        # true negatives / (true negatives + false positives)
        return np.round(self.negatives(True) / (self.negatives(True) + self.positives(False)), self.dps)


def process_results(filename='../results/results.hdf5'):
    # parses the input file and returns structures of processed results and
    # dataframes
    store = pd.HDFStore(filename, 'r')

    # These are pd.DataFrames whose columns are the model and training set
    # eg. 'uk_bert-base-uncased' means that this is the bert-base-uncased model
    # trained on the UK dataset.
    # The DataFrame rows are the test set eg. 'new_test_set' means that
    # this column corresponds to running all the models in the index on
    # new_test_set.json
    losses = store['losses'].unstack().T
    accuracies = store['accuracies'].unstack().T

    # These are DataFrames whose columns are a multi-index denoting model+training set
    # and test set.
    # These will be further processed for use, so not explaining further
    p = store['predictions']
    l = store['labels']

    # This is a nested dictionary whose first level key is the
    # model-trainingset combination eg. 'uk_bert-base-uncased'
    # and whose second level key is the test set
    results = {}

    for column in p:
        trained_model, test_set = column
        results.setdefault(trained_model, {})
        # load the test set data into a big list
        data, lbls, _ = io.get_data(os.path.join('..', 'datasets', f"{test_set}.json"))
        labels = l[column].dropna()
        assert np.allclose(np.array(lbls), labels.values)
        record = Record(trained_model, test_set,
                        p[column].dropna().values, labels.values,
                        data)
        results[trained_model][test_set] = record
        assert np.allclose(record.accuracy(), accuracies[trained_model][test_set])

    return results, losses, accuracies


if __name__ == '__main__':
    records, losses, accuracies = process_results('../results_features/results_features.hdf5')

    # sanity checks
    r = records['us_bert-base-uncased']['test_set_us_features']
    # by construction this dataset has 1500 positive, 1500 negative examples
    assert r.labels[r.labels==0].size == r.negatives(True) + r.misclassified(label=0).size
    assert r.labels[r.labels==1].size == r.positives(True) + r.misclassified(label=1).size
    assert r.positives(False) == r.misclassified(label=0).size
    assert r.negatives(False) == r.misclassified(label=1).size

    # build data structure holding misclassifications for labels 0 and 1

    tags = {0: 'not sarcastic', 1:'sarcastic'}
    trained_models = [
        'BERT-base/UK',
        'BERT-base/US',
        'BERT-base/Combined',
        'DistilBERT/UK',
        'DistilBERT/US',
        'DistilBERT/Combined',
    ]
    test_sets = ['UK', 'US', 'Combined', 'Adversarial']
    col_order = ['US', 'UK', 'Combined', 'Adversarial']
    sort_order = [(a1, a2) for a1 in trained_models for a2 in test_sets ]
    counts = {}
    stats = {}
    fps = {}
    for trained_model in records:
        for test_set in records[trained_model]:
            key = (gen_model_name(trained_model), gen_test_setname(test_set))
            record = records[trained_model][test_set]
            data = pd.Series({
                tags[label]: len(record.misclassified(label=label))
                for label in (0, 1)
            })
            counts[key] = data
            data = pd.Series({
                'precision': record.precision,
                'recall': record.recall,
                'specificity': record.specificity
            })
            stats[key] = data
            tp = record.positives(True)
            fp = record.positives(False)
            tn = record.negatives(True)
            fn = record.negatives(False)
            data = pd.Series(data=[
                fp/(fp+tn),
                fn/(fn+tp),
                tp/(tp+fn),
                tn/(tn+fp),
            ],
            index=['False positives', 'False negatives', 'True positives', 'True negatives']
            )
            fps[key] = data
    accuracies.columns = pd.Index([gen_model_name(name) for name in accuracies.columns])
    accuracies.index = pd.Index([gen_test_setname(name) for name in accuracies.index])
    accuracies = accuracies.unstack().reindex(sort_order).T.unstack()
    accuracies = accuracies[col_order]
    counts = pd.DataFrame(counts).T.reindex(sort_order)
    stats = pd.DataFrame(stats).T.reindex(sort_order)
    fps = pd.DataFrame(fps).T.reindex(sort_order)
    fps = np.round(fps, 3)

